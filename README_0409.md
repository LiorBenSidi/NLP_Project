# Narrative-to-Box-Score: A Full Statistical Synthesis Challenge

## Project Goal
This project tests a Large Language Model's (LLM) ability to synthesize a complete, structured, multi-faceted statistical report (a "box score") from a qualitative, narrative event log of a basketball game. This advanced task requires the model to perform parallel state tracking for multiple entities (teams, players) across numerous statistical categories (points, assists, fouls, etc.).

The project measures the model's ability to accurately attribute stats, perform calculations, and maintain a consistent internal state, serving as a critical test of its potential as a reliable **unstructured-to-structured** data transformation engine.

## How It Works: The Three-File Pipeline
The project operates through a clean, three-part pipeline:
1.  **`generate_data.py`:** A sophisticated game simulator that tracks a rich, nested state for every entity. It generates a new, random game log and a corresponding ground truth statistical report with perfect accuracy.
2.  **`run_eval.py`:** The orchestrator. This script takes the generated game log, constructs a detailed prompt, calls the Google Gemini API, and receives the LLM's attempt at creating the box score.
3.  **`evaluation.py`:** The judge. This script performs a deep, field-by-field comparison between the LLM's JSON output and the ground truth, calculating a final accuracy score and listing all discrepancies.

---

## Setup and Execution

Follow these steps to run the full data generation and evaluation pipeline.

### 1. Prerequisites
- A Google Gemini API Key
- Python Virtual Environment with:
    1. Python 3.10 or higher
    2. Libraries: random, json, os, re, google.generativeai, dotenv

### 2. Configure Your API Key
1. Create a new file in the root of the project named .env.
2. Inside the .env file, add your Gemini API key: GEMINI_API_KEY="your-api-key-goes-here"

### 3. Running the Pipeline
The process is a two-step command-line execution.
#### Step A: Generate a New Game Dataset
- This command runs the simulator to create a fresh game log and its corresponding ground truth report in the data/ directory.
- Run from the Command Prompt: python generate_data.py
#### Step B: Gets the output from the LLM and evaluates the accuracy compared to the ground truth.
- This command sends the newly created game log to the Gemini API and evaluates its response compared to the true stasts that we have calculate earlier.
- Run from the Command Prompt: python run_eval.py
- The script will print the API response, save the output files, and display a final accuracy score with a list of any errors found.

## **Meta-Data and Difficulty Scaling**
* **Easy:**
* **Medium:**
* **Hard:**

## Understanding the Output
After running the pipeline, the data/ directory will contain four key files:
1. examples.json: The game metadata and narrative log sent to the LLM.
2. true_report.json: The 100% accurate ground truth data generated by the simulator.
3. llm_response.txt: The raw, unmodified text string returned by the Gemini API. Essential for debugging.
4. llm_report.json: The final, valid JSON produced by the LLM after parsing and repair. This is the file that gets evaluated.

### **Meaningful Conclusion from Failure**
This design allows for a very rich analysis of failure modes in your final report:

*   **Format Errors:** The model is incapable of following structural output instructions, a fundamental failure for any API-like usage.
*   **Data Errors:** The model produces valid JSON, but the information is wrong. This is the more interesting failure. You can pinpoint specific weaknesses:
    *   **Attribution Errors:** Did it give points to the wrong player?
    *   **Aggregation Errors:** Do the player points not sum up to the team's total score?
    *   **Calculation Errors:** Was the Free Throw Percentage calculated incorrectly?
    *   **Synthesis Errors:** Did it fail to link an assist to the corresponding points?

Your conclusion could be powerful: *The model's performance as an unstructured-to-structured data engine is highly brittle. While capable of generating syntactically correct JSON, it frequently commits data errors, particularly in attributing stats from complex sentences involving multiple players. Its inability to maintain an accurate internal ledger demonstrates that it is not performing true state tracking but rather a shallow semantic association, making it unsuitable for high-fidelity data extraction and synthesis tasks that require perfect accuracy.*
